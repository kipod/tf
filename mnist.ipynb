{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://youtu.be/QogFIWa1YMg?list=PLA0M1Bcd0w8ynD1umfubKq1OBYRXhXkmH\n",
    "%env TF_CPP_MIN_LOG_LEVEL=2\n",
    "\n",
    "import tensorflow as tf  # noqa: E402\n",
    "from tensorflow.keras.datasets import mnist  # noqa: E402\n",
    "from tensorflow.keras.utils import to_categorical  # noqa: E402\n",
    "\n",
    "# Load MNIST dataset\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess data\n",
    "x_train = tf.cast(x_train.reshape((-1, 28 * 28)) / 255.0, tf.float32)\n",
    "x_test = tf.cast(x_test.reshape((-1, 28 * 28)) / 255.0, tf.float32)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNN(tf.Module):\n",
    "    \"\"\"Dense neural network\n",
    "    \"\"\"\n",
    "    def __init__(self, outputs_size: int, activate=\"relu\"):\n",
    "        \"\"\"ctor\n",
    "\n",
    "        Args:\n",
    "            outputs_size (int): count inputs in begin layer\n",
    "            activate (str, optional): activation function \"relu\" for tf.nn.relu or \"softmax\" for tf.nn.softmax . Defaults to \"relu\".\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.outputs_size = outputs_size\n",
    "        self.activate = activate\n",
    "        self.lf_init = False\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if not self.lf_init:\n",
    "            self.w = tf.random.truncated_normal([x.shape[-1], self.outputs_size], stddev=0.1, name='w')\n",
    "            self.b = tf.zeros([self.outputs_size], dtype=tf.float32, name='b')\n",
    "\n",
    "            self.w = tf.Variable(self.w)\n",
    "            self.b = tf.Variable(self.b)\n",
    "\n",
    "            self.lf_init = True\n",
    "\n",
    "        y = x @ self.w + self.b\n",
    "        if self.activate == \"relu\":\n",
    "            return tf.nn.relu(y)\n",
    "        elif self.activate == \"softmax\":\n",
    "            return tf.nn.softmax(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SequentialModule(tf.Module):\n",
    "    def __init__(self, name=\"MNIST_model\"):\n",
    "        super().__init__(name=name)\n",
    "        self.dense1 = DenseNN(128)\n",
    "        self.dense2 = DenseNN(10, activate=\"softmax\")\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.dense2(self.dense1(x))\n",
    "\n",
    "model = SequentialModule()\n",
    "\n",
    "# model.submodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_pred, y_true):\n",
    "    loss = tf.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# opt = tf.optimizers.Adam(learning_rate=0.001)\n",
    "# for M1/M2 Macs architecture use legacy optimizer\n",
    "opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.0001)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "TOTAL = x_train.shape[0]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)\n",
    "\n",
    "@tf.function\n",
    "def train_batch(x_batch, y_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "            f_loss = cross_entropy(model(x_batch), y_batch)\n",
    "\n",
    "    grad = tape.gradient(f_loss, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grad, model.trainable_variables))\n",
    "    return f_loss\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss = 0.0\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        loss += train_batch(x_batch, y_batch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} loss {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "\n",
    "y_pred = model(x_test)\n",
    "y2 = tf.argmax(y_pred, axis=1).numpy()\n",
    "acc = len(y_test[y2 == y_test]) / y_test.shape[0] * 100\n",
    "print(f\"Accuracy: {acc:.2f}%\")\n",
    "\n",
    "# # Get accuracy by Tensorflow\n",
    "# acc = tf.metrics.Accuracy()\n",
    "# acc.update_state(y_test, y2)\n",
    "# print(f\"Accuracy: {acc.result().numpy() * 100:.2f}%\")\n",
    "\n",
    "# Save model\n",
    "# tf.saved_model.save(layer_1, \"layer_1\")\n",
    "# tf.saved_model.save(layer_2, \"layer_2\")\n",
    "tf.saved_model.save(model, \"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
